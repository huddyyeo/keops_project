{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nystrom_tests_incl_block_sparse.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWuFPkopIETu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d1c289e-82fa-4c7d-d86c-263ff66b2e44"
      },
      "source": [
        "!pip install pykeops[full] > log.log\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: breathe 4.26.1 has requirement Sphinx<3.5,>=3.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7Xkl02QMsr_"
      },
      "source": [
        "# To showcase and share test results\n",
        "\n",
        "Please don't keep all the dirty work here, just clean results that may be useful to share."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xhwIA_wM4BY"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.utils import check_random_state, as_float_array\n",
        "from scipy.linalg import svd\n",
        "from pykeops.torch import LazyTensor\n",
        "from sklearn.kernel_approximation import Nystroem\n",
        "import scipy\n",
        "\n",
        "from pykeops.numpy import LazyTensor as LazyTensor_n\n",
        "from pykeops.numpy.cluster import grid_cluster\n",
        "from pykeops.numpy.cluster import cluster_ranges_centroids\n",
        "from pykeops.numpy.cluster import sort_clusters\n",
        "from pykeops.numpy.cluster import from_matrix\n",
        "from scipy.sparse.linalg import aslinearoperator, eigsh"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ql1vxZwNPwP"
      },
      "source": [
        "# Basic Nystrom code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "421dQQUDM-jD"
      },
      "source": [
        "##############################################################################\n",
        "\n",
        "'''\n",
        "The two classes below implement the Nystrom algorithm. One can transform\n",
        "the data into the approximated feature-space and/or obtain the approximated \n",
        "kernel.\n",
        "\n",
        "Example of usage:\n",
        "\n",
        "Let X_i be a LazyTensor of shape =  (1, length, features), then \n",
        "\n",
        "LN = LazyNystrom_T(n_components=100 ,kernel='rbf', gamma=1.) # creates an instance\n",
        "LN.fit(X_i)  # fits to data         \n",
        "X_new_i = LN.transform(X_i)  # transform data to approximated features\n",
        "K_approx = LN.K_approx(X_i)  # obtain approximated kernel\n",
        "\n",
        "'''\n",
        "\n",
        "class LazyNystrom_N:\n",
        "    '''\n",
        "        Class to implement Nystrom on torch LazyTensors.\n",
        "        This class works as an interface between lazy tensors and \n",
        "        the Nystrom algorithm in NumPy.\n",
        "\n",
        "        * The fit method computes K^{-1}_q.\n",
        "\n",
        "        * The transform method maps the data into the feature space underlying\n",
        "        the Nystrom-approximated kernel.\n",
        "\n",
        "        * The method K_approx directly computes the Nystrom approximation.\n",
        "\n",
        "        Parameters:\n",
        "\n",
        "        n_components [int] = how many samples to select from data.\n",
        "        kernel [str] = type of kernel to use. Current options = {linear, rbf}.\n",
        "        gamma [float] = exponential constant for the RBF kernel. \n",
        "        random_state=[None, float] = to set a random seed for the random\n",
        "                                     sampling of the samples. To be used when \n",
        "                                     reproducibility is needed.\n",
        "\n",
        "    '''\n",
        "  \n",
        "    def __init__(self, n_components=100, kernel='linear', gamma:float = 1., \n",
        "                 random_state=None): \n",
        "\n",
        "        self.n_components = n_components\n",
        "        self.kernel = kernel\n",
        "        self.random_state = random_state\n",
        "        self.gamma = gamma\n",
        "\n",
        "\n",
        "    def fit(self, X:LazyTensor):\n",
        "        ''' \n",
        "        Args:   X = lazy tensor with features of shape \n",
        "                (1, n_samples, n_features)\n",
        "\n",
        "        Returns: Fitted instance of the class\n",
        "        '''\n",
        "\n",
        "        # Basic checks\n",
        "        assert type(X) == LazyTensor, 'Input to fit(.) must be a LazyTensor.'\n",
        "        assert X.shape[1] >= self.n_components, f'The application needs X.shape[1] >= n_components.'\n",
        "\n",
        "        X = X.sum(dim=0).numpy()\n",
        "        # Number of samples\n",
        "        n_samples = X.shape[0]\n",
        "        # Define basis\n",
        "        rnd = check_random_state(self.random_state)\n",
        "        inds = rnd.permutation(n_samples)\n",
        "        basis_inds = inds[:self.n_components]\n",
        "        basis = X[basis_inds]\n",
        "        # Build smaller kernel\n",
        "        basis_kernel = self._pairwise_kernels(basis, kernel = self.kernel)  \n",
        "        # Get SVD\n",
        "        U, S, V = svd(basis_kernel)\n",
        "        S = np.maximum(S, 1e-12)\n",
        "        self.normalization_ = np.dot(U / np.sqrt(S), V)\n",
        "        self.components_ = basis\n",
        "        self.component_indices_ = inds\n",
        "        return self\n",
        "\n",
        "\n",
        "    def _pairwise_kernels(self, x:np.array, y:np.array = None, kernel='linear',\n",
        "                          gamma = 1.):\n",
        "        '''Helper function to build kernel\n",
        "        \n",
        "        Args:   X = torch tensor of dimension 2.\n",
        "                K_type = type of Kernel to return\n",
        "        '''\n",
        "        \n",
        "        if y is None:\n",
        "            y = x\n",
        "        if kernel == 'linear':\n",
        "            K = x @ y.T \n",
        "        elif kernel == 'rbf':\n",
        "            K =  ( (x[:,None,:] - y[None,:,:])**2 ).sum(-1)\n",
        "            K = np.exp(- gamma* K)\n",
        "  \n",
        "        return K\n",
        "\n",
        "    def transform(self, X:LazyTensor) -> LazyTensor:\n",
        "        ''' Applies transform on the data.\n",
        "        \n",
        "        Args:\n",
        "            X [LazyTensor] = data to transform\n",
        "        Returns\n",
        "            X [LazyTensor] = data after transformation\n",
        "        '''\n",
        "        \n",
        "        X = X.sum(dim=0)\n",
        "        K_nq = self._pairwise_kernels(X, self.components_, self.kernel)\n",
        "\n",
        "        return LazyTensor((K_nq @ self.normalization_.T)[None,:,:])\n",
        "\n",
        "    \n",
        "    def K_approx(self, X:LazyTensor) -> LazyTensor:\n",
        "        ''' Function to return Nystrom approximation to the kernel.\n",
        "        \n",
        "        Args:\n",
        "            X[LazyTensor] = data used in fit(.) function.\n",
        "        Returns\n",
        "            K[LazyTensor] = Nystrom approximation to kernel'''\n",
        "        \n",
        "        X = X.sum(dim=0).numpy()\n",
        "        K_nq = self._pairwise_kernels(X, self.components_, self.kernel)\n",
        "        K_approx = K_nq @ self.normalization_ @ K_nq.T\n",
        "        K_approx = torch.tensor(K_approx)\n",
        "        return LazyTensor(K_approx[None,:,:])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "\n",
        "# Same as LazyNystrom_N but written with Pytorch\n",
        "\n",
        "class LazyNystrom_T:\n",
        "    '''\n",
        "        Class to implement Nystrom on torch LazyTensors.\n",
        "        This class works as an interface between lazy tensors and \n",
        "        the Nystrom algorithm in NumPy.\n",
        "\n",
        "        * The fit method computes K^{-1}_q.\n",
        "\n",
        "        * The transform method maps the data into the feature space underlying\n",
        "        the Nystrom-approximated kernel.\n",
        "\n",
        "        * The method K_approx directly computes the Nystrom approximation.\n",
        "\n",
        "        Parameters:\n",
        "\n",
        "        n_components [int] = how many samples to select from data.\n",
        "        kernel [str] = type of kernel to use. Current options = {linear, rbf}.\n",
        "        gamma [float] = exponential constant for the RBF kernel. \n",
        "        random_state=[None, float] = to set a random seed for the random\n",
        "                                     sampling of the samples. To be used when \n",
        "                                     reproducibility is needed.\n",
        "\n",
        "    '''\n",
        "  \n",
        "    def __init__(self, n_components=100, kernel='linear',  gamma:float = 1., \n",
        "                 random_state=None ):\n",
        "        \n",
        "        self.n_components = n_components\n",
        "        self.kernel = kernel\n",
        "        self.random_state = random_state\n",
        "        self.gamma = gamma\n",
        "\n",
        "\n",
        "    def fit(self, X:LazyTensor):\n",
        "        ''' \n",
        "        Args:   X = torch lazy tensor with features of shape \n",
        "                (1, n_samples, n_features)\n",
        "\n",
        "        Returns: Fitted instance of the class\n",
        "        '''\n",
        "\n",
        "        # Basic checks: we have a lazy tensor and n_components isn't too large\n",
        "        assert type(X) == LazyTensor, 'Input to fit(.) must be a LazyTensor.'\n",
        "        assert X.shape[1] >= self.n_components, f'The application needs X.shape[1] >= n_components.'\n",
        "\n",
        "        X = X.sum(dim=0) \n",
        "        # Number of samples\n",
        "        n_samples = X.size(0)\n",
        "        # Define basis\n",
        "        rnd = check_random_state(self.random_state)\n",
        "        inds = rnd.permutation(n_samples)\n",
        "        basis_inds = inds[:self.n_components]\n",
        "        basis = X[basis_inds]\n",
        "        # Build smaller kernel\n",
        "        basis_kernel = self._pairwise_kernels(basis, kernel = self.kernel)  \n",
        "        # Get SVD\n",
        "        U, S, V = torch.svd(basis_kernel)\n",
        "        S = torch.maximum(S, torch.ones(S.size()) * 1e-12)\n",
        "        self.normalization_ = torch.mm(U / np.sqrt(S), V.t())\n",
        "        self.components_ = basis\n",
        "        self.component_indices_ = inds\n",
        "        \n",
        "        return self\n",
        "\n",
        "\n",
        "    def _pairwise_kernels(self, x:torch.tensor, y:torch.tensor = None, kernel='linear',\n",
        "                          gamma = 1.) -> torch.tensor:\n",
        "        '''Helper function to build kernel\n",
        "        \n",
        "        Args:   X = torch tensor of dimension 2.\n",
        "                K_type = type of Kernel to return\n",
        "        '''\n",
        "        \n",
        "        if y is None:\n",
        "            y = x\n",
        "        if kernel == 'linear':\n",
        "            K = x @ y.T\n",
        "        elif kernel == 'rbf':\n",
        "            K =  ( (x[:,None,:] - y[None,:,:])**2 ).sum(-1)\n",
        "            K = torch.exp(- gamma * K )\n",
        "\n",
        "        return K\n",
        "\n",
        "    def transform(self, X:LazyTensor) -> LazyTensor:\n",
        "        ''' Applies transform on the data.\n",
        "        \n",
        "        Args:\n",
        "            X [LazyTensor] = data to transform\n",
        "        Returns\n",
        "            X [LazyTensor] = data after transformation\n",
        "        '''\n",
        "        \n",
        "        X = X.sum(dim=0)\n",
        "        K_nq = self._pairwise_kernels(X, self.components_, self.kernel)\n",
        "        return LazyTensor((K_nq @ self.normalization_.t())[None,:,:])\n",
        "\n",
        "    \n",
        "    def K_approx(self, X:LazyTensor) -> LazyTensor:\n",
        "        ''' Function to return Nystrom approximation to the kernel.\n",
        "        \n",
        "        Args:\n",
        "            X[LazyTensor] = data used in fit(.) function.\n",
        "        Returns\n",
        "            K[LazyTensor] = Nystrom approximation to kernel'''\n",
        "        \n",
        "        X = X.sum(dim=0)\n",
        "        K_nq = self._pairwise_kernels(X, self.components_, self.kernel)\n",
        "        K_approx = K_nq @ self.normalization_ @ K_nq.t()\n",
        "        return LazyTensor(K_approx[None,:,:])\n",
        "\n",
        "  \n",
        "##########################################################################\n",
        "\n",
        "# Similar to the above but utilizing KeOps\n",
        "\n",
        "class Nystrom_NK:\n",
        "    '''\n",
        "        Class to implement Nystrom using numpy and PyKeops.\n",
        "        * The fit method computes K^{-1}_q.\n",
        "        * The transform method maps the data into the feature space underlying\n",
        "        the Nystrom-approximated kernel.\n",
        "        * The method K_approx directly computes the Nystrom approximation.\n",
        "        Parameters:\n",
        "        n_components [int] = how many samples to select from data.\n",
        "        kernel [str] = type of kernel to use. Current options = {rbf}.\n",
        "        sigma [float] = exponential constant for the RBF kernel. \n",
        "        eps[float] = size for square bins\n",
        "        random_state=[None, float] = to set a random seed for the random\n",
        "                                     sampling of the samples. To be used when \n",
        "                                     reproducibility is needed.\n",
        "    '''\n",
        "  \n",
        "    def __init__(self, n_components=100, kernel='rbf', sigma:float = 1., \n",
        "                 eps:float = 0.05, random_state=None): \n",
        "\n",
        "        self.n_components = n_components\n",
        "        self.kernel = kernel\n",
        "        self.random_state = random_state\n",
        "        self.sigma = sigma\n",
        "        self.eps = eps\n",
        "\n",
        "\n",
        "    def fit(self, x:np.ndarray):\n",
        "        ''' \n",
        "        Args:   x = numpy array of shape (n_samples, n_features)\n",
        "        Returns: Fitted instance of the class\n",
        "        '''\n",
        "\n",
        "        # Basic checks\n",
        "        assert type(x) == np.ndarray, 'Input to fit(.) must be an array.'\n",
        "        assert x.shape[0] >= self.n_components, f'The application needs X.shape[0] >= n_components.'\n",
        "\n",
        "        # Number of samples\n",
        "        n_samples = x.shape[0]\n",
        "        # Define basis\n",
        "        rnd = check_random_state(self.random_state)\n",
        "        inds = rnd.permutation(n_samples) \n",
        "        basis_inds = inds[:self.n_components] \n",
        "        basis = x[basis_inds]\n",
        "        # Build smaller kernel\n",
        "        basis_kernel = self._pairwise_kernels(basis, kernel = self.kernel)  \n",
        "        # Spectral decomposition\n",
        "        S, U = self._spectral(basis_kernel)\n",
        "        S = np.maximum(S, 1e-12)\n",
        "        self.normalization_ = np.dot(U / np.sqrt(S), U.T)\n",
        "        self.components_ = basis\n",
        "        self.component_indices_ = inds\n",
        "        return self\n",
        "\n",
        "\n",
        "    def _pairwise_kernels(self, x:np.array, y:np.array = None, kernel='rbf',\n",
        "                          sigma = 1.) -> LazyTensor:\n",
        "        '''Helper function to build kernel\n",
        "        \n",
        "        Args:   X = torch tensor of dimension 2,\n",
        "                K_type = type of Kernel to return.\n",
        "        Returns:\n",
        "                K_ij[LazyTensor]\n",
        "        '''\n",
        "        if y is None:\n",
        "            y = x\n",
        "        if kernel == 'linear': \n",
        "            K_ij = x @ y.T \n",
        "        elif kernel == 'rbf':\n",
        "            x /= sigma\n",
        "            y /= sigma\n",
        "            x_i, x_j = LazyTensor_n(x[:, None, :]), LazyTensor_n(y[None, :, :])\n",
        "            K_ij = (-1*((x_i - x_j)**2).sum(2)).exp()\n",
        "            # block-sparse reduction preprocess\n",
        "            K_ij = self._Gauss_block_sparse_pre(x, y, K_ij, self.sigma, self.eps)\n",
        "        elif kernel == 'exp':\n",
        "            x_i, x_j = LazyTensor_n(x[:, None, :]), LazyTensor_n(y[None, :, :])\n",
        "            K_ij = (-1 * (abs(x_i - x_j)).sum(2)).exp()\n",
        "            # block-sparse reduction preprocess\n",
        "            K_ij = self._Gauss_block_sparse_pre(x, y, K_ij, self.sigma, self.eps)\n",
        "        return K_ij\n",
        "\n",
        "\n",
        "    def _spectral(self, X_i:LazyTensor):\n",
        "        '''\n",
        "        Helper function to compute eigendecomposition of K_q.\n",
        "        Args: X_i[numpy LazyTensor]\n",
        "        Returns S[np.array] eigenvalues,\n",
        "                U[np.array] eigenvectors\n",
        "        '''\n",
        "        K_linear = aslinearoperator(X_i)\n",
        "        k = K_linear.shape[0] - 1\n",
        "        S, U = eigsh(K_linear, k=k, which='LM')\n",
        "        return S, U\n",
        "        \n",
        "\n",
        "    def transform(self, x:np.ndarray) -> np.array:\n",
        "        ''' Applies transform on the data.\n",
        "        \n",
        "        Args:\n",
        "            X [np.array] = data to transform\n",
        "        Returns\n",
        "            X [np.array] = data after transformation\n",
        "        '''\n",
        "        \n",
        "        K_nq = self._pairwise_kernels(x, self.components_, self.kernel)\n",
        "        x_new = K_nq @ self.normalization_.T\n",
        "        return x_new\n",
        "\n",
        "    \n",
        "    def K_approx(self, x:np.array) -> np.array:\n",
        "        ''' Function to return Nystrom approximation to the kernel.\n",
        "        \n",
        "        Args:\n",
        "            X[np.array] = data used in fit(.) function.\n",
        "        Returns\n",
        "            K[np.array] = Nystrom approximation to kernel'''\n",
        "       \n",
        "        K_nq = self._pairwise_kernels(x, self.components_, self.kernel)\n",
        "        # For arrays: K_approx = K_nq @ self.normalization_ @ K_nq.T\n",
        "        # But to use @ with lazy tensors we have:\n",
        "        K_approx = K_nq @ (K_nq @ self.normalization_ ).T\n",
        "        \n",
        "        return K_approx.T \n",
        "\n",
        "\n",
        "    def _Gauss_block_sparse_pre(self, x:np.array, y:np.array, K_ij:LazyTensor, \n",
        "                               sigma:float = 1., eps:float = 0.05):\n",
        "        ''' \n",
        "        Helper function to preprocess data for block-sparse reduction\n",
        "        of the Gaussian kernel\n",
        "    \n",
        "        Args: \n",
        "            x[np.array], y[np.array] = arrays giving rise to Gaussian kernel K(x,y)\n",
        "            K_ij[LazyTensor_n] = symbolic representation of K(x,y)\n",
        "            eps[float] = size for square bins\n",
        "        Returns:\n",
        "            K_ij[LazyTensor_n] = symbolic representation of K(x,y) with \n",
        "                                set sparse ranges\n",
        "        '''\n",
        "\n",
        "        # class labels\n",
        "        x_labels = grid_cluster(x, eps) \n",
        "        y_labels = grid_cluster(y, eps) \n",
        "        # compute one range and centroid per class\n",
        "        x_ranges, x_centroids, _ = cluster_ranges_centroids(x, x_labels)\n",
        "        y_ranges, y_centroids, _ = cluster_ranges_centroids(y, y_labels)\n",
        "        # sort points\n",
        "        x, x_labels = sort_clusters(x, x_labels)\n",
        "        y, y_labels = sort_clusters(y, y_labels) \n",
        "        # Compute a coarse Boolean mask:\n",
        "        D = np.sum((x_centroids[:, None, :] - y_centroids[None, :, :]) ** 2, 2)\n",
        "        keep = D < (4 * sigma) ** 2  # self.sigma \n",
        "        # mask -> set of integer tensors\n",
        "        ranges_ij = from_matrix(x_ranges, y_ranges, keep)\n",
        "        K_ij.ranges = ranges_ij  # block-sparsity pattern\n",
        "\n",
        "        return K_ij\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bh7DHDWND2E"
      },
      "source": [
        "## Testing the fit and transform methods - numpy version\n",
        "\n",
        "Note: Given $a$ and $b$ two vectors, I am computing the error as $$e = \\frac{\\| a- b\\|_2}{L} $$ where $L = len(a)$. My reasoning for the $1/L$ factor is that the error will grow linearly with the size of the vectors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jLOzI1QNWdj",
        "outputId": "e15d9c21-0ab5-40e1-8c83-173da7fce7d6"
      },
      "source": [
        "# We test the LazyNystrom_N fit/transform methods using a Linear kernel\n",
        "\n",
        "length = 1000\n",
        "num_sampling = 100\n",
        "\n",
        "x = torch.randint(10,(1,length,3),dtype=torch.float32)\n",
        "X_i = LazyTensor(x)\n",
        "\n",
        "# Instatiate & fit Nystroem for comparison\n",
        "sk_N = Nystroem(kernel='linear', n_components=num_sampling, random_state=0).fit(x[0].numpy())  # input: (length, features) array\n",
        "x_new = sk_N.transform(x[0].numpy())                                                           # output: (length, num_sampling) array\n",
        "\n",
        "# Instatiate & fit on lazy tensor version\n",
        "LN_test = LazyNystrom_N(num_sampling, random_state=0).fit(X_i)   # input: (1, length, features) lazy tensor\n",
        "X_new_i = LN_test.transform(X_i)                                 # output: (1,length,num_sampling) lazy tensor\n",
        "\n",
        "# Print the L2 error\n",
        "err = np.linalg.norm(x_new - X_new_i.sum(dim=0).numpy()) / x_new.size\n",
        "print(f'Error when compared to sklearn = {err}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compiling libKeOpstorch31970816d1 in /root/.cache/pykeops-1.4.2-cpython-36:\n",
            "       formula: Sum_Reduction(Var(0,3,1),1)\n",
            "       aliases: Var(0,3,1); \n",
            "       dtype  : float32\n",
            "... Done.\n",
            "Compiling libKeOpstorcha99f5d65b6 in /root/.cache/pykeops-1.4.2-cpython-36:\n",
            "       formula: Sum_Reduction(Var(0,100,1),1)\n",
            "       aliases: Var(0,100,1); \n",
            "       dtype  : float32\n",
            "... Done.\n",
            "Error when compared to sklearn = 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WZo3i-9NcB0",
        "outputId": "5896b488-5106-451e-b837-3a0bc580140b"
      },
      "source": [
        "# We test the LazyNystrom_N fit/transform methods using a Gaussian kernel\n",
        "\n",
        "length = 1000\n",
        "num_sampling = 100\n",
        "\n",
        "x = torch.randint(10,(1,length,3),dtype=torch.float32)\n",
        "X_i = LazyTensor(x)\n",
        "\n",
        "# Instatiate & fit Nystroem for comparison\n",
        "sk_N = Nystroem(kernel='rbf', gamma=1., n_components=num_sampling, random_state=0).fit(x[0].numpy())\n",
        "x_new = sk_N.transform(x[0].numpy())      # (length, num_sampling) array\n",
        "\n",
        "# Instatiate & fit on lazy tensor version\n",
        "LN_test = LazyNystrom_N(num_sampling,kernel='rbf', gamma=1., random_state=0).fit(X_i) # input: (1, length, features) lazy tensor\n",
        "X_new_i = LN_test.transform(X_i)                                                      # output: (1,length,num_sampling) lazy tensor\n",
        "\n",
        "# Print the L2 error\n",
        "err = np.linalg.norm(x_new - X_new_i.sum(dim=0).numpy()) / x_new.size\n",
        "print(f'Error when compared to sklearn =  {err}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error when compared to sklearn =  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNyU-oGFNocH"
      },
      "source": [
        "## Testing the fit and transform methods - torch version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AqnQq0cNtFX",
        "outputId": "e8b3e433-8926-48af-a23f-7fa928b58ef8"
      },
      "source": [
        "# We test the LazyNystrom_T fit/transform methods using a Linear kernel\n",
        "\n",
        "length = 1000\n",
        "num_sampling = 100\n",
        "\n",
        "x = torch.randint(10,(1,length,3),dtype=torch.float32)\n",
        "X_i = LazyTensor(x)\n",
        "\n",
        "# Instatiate & fit Nystroem for comparison\n",
        "sk_N = Nystroem(kernel='linear', n_components=num_sampling, random_state=0).fit(x[0].numpy())\n",
        "x_new = sk_N.transform(x[0].numpy())      # (length, num_sampling) array\n",
        "\n",
        "# Instatiate & fit on lazy tensor version\n",
        "LN_test = LazyNystrom_T(num_sampling, random_state=0).fit(X_i)                   # input: (1, length, features) lazy tensor\n",
        "X_new_i = LN_test.transform(X_i)                                                 # output: (1,length,num_sampling) lazy tensor\n",
        "\n",
        "# Print the L2 error\n",
        "err = np.linalg.norm(x_new - X_new_i.sum(dim=0).numpy()) / x_new.size\n",
        "print(f'Error when compared to sklearn = {err}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error when compared to sklearn = 6.183534264564514e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z5-PKfFNxsa",
        "outputId": "f91d29a0-8588-4565-9e0c-45701f74aee9"
      },
      "source": [
        "# We test the LazyNystrom_T fit/transform methods using a Gaussian kernel\n",
        "\n",
        "length = 1000\n",
        "num_sampling = 100\n",
        "\n",
        "x = torch.randint(10,(1,length,3),dtype=torch.float32)\n",
        "print(x)\n",
        "X_i = LazyTensor(x)\n",
        "\n",
        "# Instatiate & fit Nystroem for comparison\n",
        "sk_N = Nystroem(kernel='rbf', n_components=num_sampling, random_state=0).fit(x[0].numpy())\n",
        "x_new = sk_N.transform(x[0].numpy())      # (length, num_sampling) array\n",
        "\n",
        "# Instatiate & fit on lazy tensor version\n",
        "LN_test = LazyNystrom_T(num_sampling, kernel='rbf', random_state=0).fit(X_i)\n",
        "X_new_i = LN_test.transform(X_i)          # (1,length,num_sampling) lazy tensor\n",
        "\n",
        "# Print the L2 error\n",
        "err = np.linalg.norm(x_new - X_new_i.sum(dim=0).numpy()) / x_new.size\n",
        "print(f'Error when compared to sklearn = {err}')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[7., 6., 3.],\n",
            "         [2., 5., 4.],\n",
            "         [6., 0., 2.],\n",
            "         ...,\n",
            "         [2., 8., 4.],\n",
            "         [1., 9., 2.],\n",
            "         [2., 1., 7.]]])\n",
            "Error when compared to sklearn = 0.00013295495986938476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1E-9ypAcbVX"
      },
      "source": [
        "#Testing the methods of the LazyNystrom_NK class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIePdKCYupKc",
        "outputId": "114a4be9-a9f3-4162-9c8b-eabde67e7ac2"
      },
      "source": [
        "# Testing the fit/transform method with linear kernel\r\n",
        "\r\n",
        "length = 1000\r\n",
        "num_sampling = 100\r\n",
        "\r\n",
        "X = np.random.uniform(0,100,size = (length,3))\r\n",
        "\r\n",
        "# ours\r\n",
        "nystrom = Nystrom_NK(kernel = 'linear', n_components = num_sampling, random_state = 0)\r\n",
        "n_fit = nystrom.fit(X)\r\n",
        "x_new = nystrom.transform(X)\r\n",
        "\r\n",
        "\r\n",
        "# sklearn\r\n",
        "\r\n",
        "sk_n = Nystroem(kernel='linear', gamma=1., n_components=num_sampling, random_state=0).fit(X)\r\n",
        "X_new_sk = sk_N.transform(X)      # (length, num_sampling) array\r\n",
        "\r\n",
        "# Print the L2 error\r\n",
        "err = np.linalg.norm(x_new - X_new_sk) / X_new_sk.size\r\n",
        "print(f'Error when compared to sklearn =  {err}')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error when compared to sklearn =  0.03129212019556455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODAGRU7Rvpow"
      },
      "source": [
        "There was a typo in the code! (in pairwise kernels -> linear kernel) Fix in master"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ihwunzVN2TE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "449bc52c-8ad3-41c2-b424-1848758700ea"
      },
      "source": [
        "# Testing the fit/transform method with linear kernel\r\n",
        "\r\n",
        "length = 1000\r\n",
        "num_sampling = 100\r\n",
        "\r\n",
        "X = np.random.uniform(0,100,size = (length,3))\r\n",
        "\r\n",
        "# ours\r\n",
        "nystrom = Nystrom_NK(random_state = 0)\r\n",
        "n_fit = nystrom.fit(X)\r\n",
        "x_new = nystrom.transform(X)\r\n",
        "\r\n",
        "\r\n",
        "# sklearn\r\n",
        "\r\n",
        "sk_n = Nystroem(kernel='rbf', gamma=1., n_components=num_sampling, random_state=0).fit(X)\r\n",
        "X_new_sk = sk_N.transform(X)      # (length, num_sampling) array\r\n",
        "\r\n",
        "# Print the L2 error\r\n",
        "err = np.linalg.norm(x_new - X_new_sk) / X_new_sk.size\r\n",
        "print(f'Error when compared to sklearn =  {err}')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error when compared to sklearn =  9.045198995833238e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8yXK0-hpSWS",
        "outputId": "86727fff-3d60-45fe-be84-4c1eba983a08"
      },
      "source": [
        "# Testing the fit/transform method with linear kernel\r\n",
        "\r\n",
        "length = 1000\r\n",
        "num_sampling = 100\r\n",
        "\r\n",
        "X = np.random.uniform(0,100,size = (length,3))\r\n",
        "\r\n",
        "# ours\r\n",
        "nystrom = Nystrom_NK(kernel = 'exp', random_state = 0)\r\n",
        "n_fit = nystrom.fit(X)\r\n",
        "x_new = nystrom.transform(X)\r\n",
        "\r\n",
        "\r\n",
        "# sklearn\r\n",
        "\r\n",
        "# the resulting difference is low because exp and rbf are so similar but need to\r\n",
        "# change the sklearn kernel to exp lol\r\n",
        "sk_n = Nystroem(kernel='rbf', gamma=1., n_components=num_sampling, random_state=0).fit(X)\r\n",
        "X_new_sk = sk_N.transform(X)      # (length, num_sampling) array\r\n",
        "\r\n",
        "# Print the L2 error\r\n",
        "err = np.linalg.norm(x_new - X_new_sk) / X_new_sk.size\r\n",
        "print(f'Error when compared to sklearn =  {err}')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error when compared to sklearn =  8.344281579492499e-06\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}