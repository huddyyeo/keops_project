{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Nystrom_tests.ipynb","provenance":[{"file_id":"1VAfNUmXg4KHcHsyWIVTj2JXuBVl72MZ1","timestamp":1612092353520}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"sWuFPkopIETu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612092580454,"user_tz":-120,"elapsed":18995,"user":{"displayName":"Yaniel Cabrera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnuKAiqQmMSKvIwoiQyk5G_g5QALh-semcxgjuQ6o=s64","userId":"16420725285670537399"}},"outputId":"6b91816d-eacb-4c0e-d8c8-faa0807bf9f9"},"source":["!pip install pykeops[full] > log.log\r\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[31mERROR: breathe 4.26.1 has requirement Sphinx<3.5,>=3.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s7Xkl02QMsr_"},"source":["# To showcase and share test results\n","\n","Please don't keep all the dirty work here, just clean results that may be useful to share."]},{"cell_type":"code","metadata":{"id":"9xhwIA_wM4BY","executionInfo":{"status":"ok","timestamp":1612092581325,"user_tz":-120,"elapsed":19860,"user":{"displayName":"Yaniel Cabrera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnuKAiqQmMSKvIwoiQyk5G_g5QALh-semcxgjuQ6o=s64","userId":"16420725285670537399"}}},"source":["import numpy as np\n","import torch\n","from sklearn.utils import check_random_state, as_float_array\n","from scipy.linalg import svd\n","from pykeops.torch import LazyTensor\n","from sklearn.kernel_approximation import Nystroem\n","import scipy"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Ql1vxZwNPwP"},"source":["# Basic Nystrom code"]},{"cell_type":"code","metadata":{"id":"421dQQUDM-jD","executionInfo":{"status":"ok","timestamp":1612092582154,"user_tz":-120,"elapsed":20686,"user":{"displayName":"Yaniel Cabrera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnuKAiqQmMSKvIwoiQyk5G_g5QALh-semcxgjuQ6o=s64","userId":"16420725285670537399"}}},"source":["##############################################################################\n","\n","'''\n","The two classes below implement the Nystrom algorithm. One can transform\n","the data into the approximated feature-space and/or obtain the approximated \n","kernel.\n","\n","Example of usage:\n","\n","Let X_i be a LazyTensor of shape =  (1, length, features), then \n","\n","LN = LazyNystrom_T(n_components=100 ,kernel='rbf', gamma=1.) # creates an instance\n","LN.fit(X_i)  # fits to data         \n","X_new_i = LN.transform(X_i)  # transform data to approximated features\n","K_approx = LN.K_approx(X_i)  # obtain approximated kernel\n","\n","'''\n","\n","class LazyNystrom_N:\n","    '''\n","        Class to implement Nystrom on torch LazyTensors.\n","        This class works as an interface between lazy tensors and \n","        the Nystrom algorithm in NumPy.\n","\n","        * The fit method computes K^{-1}_q.\n","\n","        * The transform method maps the data into the feature space underlying\n","        the Nystrom-approximated kernel.\n","\n","        * The method K_approx directly computes the Nystrom approximation.\n","\n","        Parameters:\n","\n","        n_components [int] = how many samples to select from data.\n","        kernel [str] = type of kernel to use. Current options = {linear, rbf}.\n","        gamma [float] = exponential constant for the RBF kernel. \n","        random_state=[None, float] = to set a random seed for the random\n","                                     sampling of the samples. To be used when \n","                                     reproducibility is needed.\n","\n","    '''\n","  \n","    def __init__(self, n_components=100, kernel='linear', gamma:float = 1., \n","                 random_state=None): \n","\n","        self.n_components = n_components\n","        self.kernel = kernel\n","        self.random_state = random_state\n","        self.gamma = gamma\n","\n","\n","    def fit(self, X:LazyTensor):\n","        ''' \n","        Args:   X = lazy tensor with features of shape \n","                (1, n_samples, n_features)\n","\n","        Returns: Fitted instance of the class\n","        '''\n","\n","        # Basic checks\n","        assert type(X) == LazyTensor, 'Input to fit(.) must be a LazyTensor.'\n","        assert X.shape[1] >= self.n_components, f'The application needs X.shape[1] >= n_components.'\n","\n","        X = X.sum(dim=0).numpy()\n","        # Number of samples\n","        n_samples = X.shape[0]\n","        # Define basis\n","        rnd = check_random_state(self.random_state)\n","        inds = rnd.permutation(n_samples)\n","        basis_inds = inds[:self.n_components]\n","        basis = X[basis_inds]\n","        # Build smaller kernel\n","        basis_kernel = self._pairwise_kernels(basis, kernel = self.kernel)  \n","        # Get SVD\n","        U, S, V = svd(basis_kernel)\n","        S = np.maximum(S, 1e-12)\n","        self.normalization_ = np.dot(U / np.sqrt(S), V)\n","        self.components_ = basis\n","        self.component_indices_ = inds\n","        return self\n","\n","\n","    def _pairwise_kernels(self, x:np.array, y:np.array = None, kernel='linear',\n","                          gamma = 1.):\n","        '''Helper function to build kernel\n","        \n","        Args:   X = torch tensor of dimension 2.\n","                K_type = type of Kernel to return\n","        '''\n","        \n","        if y is None:\n","            y = x\n","        if kernel == 'linear':\n","            K = x @ y.T \n","        elif kernel == 'rbf':\n","            K =  ( (x[:,None,:] - y[None,:,:])**2 ).sum(-1)\n","            K = np.exp(- gamma* K)\n","  \n","        return K\n","\n","    def transform(self, X:LazyTensor) -> LazyTensor:\n","        ''' Applies transform on the data.\n","        \n","        Args:\n","            X [LazyTensor] = data to transform\n","        Returns\n","            X [LazyTensor] = data after transformation\n","        '''\n","        \n","        X = X.sum(dim=0)\n","        K_nq = self._pairwise_kernels(X, self.components_, self.kernel)\n","\n","        return LazyTensor((K_nq @ self.normalization_.T)[None,:,:])\n","\n","    \n","    def K_approx(self, X:LazyTensor) -> LazyTensor:\n","        ''' Function to return Nystrom approximation to the kernel.\n","        \n","        Args:\n","            X[LazyTensor] = data used in fit(.) function.\n","        Returns\n","            K[LazyTensor] = Nystrom approximation to kernel'''\n","        \n","        X = X.sum(dim=0).numpy()\n","        K_nq = self._pairwise_kernels(X, self.components_, self.kernel)\n","        K_approx = K_nq @ self.normalization_ @ K_nq.T\n","        K_approx = torch.tensor(K_approx)\n","        return LazyTensor(K_approx[None,:,:])\n","\n","\n","\n","\n","##########################################################################\n","\n","# Same as LazyNystrom_N but written with Pytorch\n","\n","class LazyNystrom_T:\n","    '''\n","        Class to implement Nystrom on torch LazyTensors.\n","        This class works as an interface between lazy tensors and \n","        the Nystrom algorithm in NumPy.\n","\n","        * The fit method computes K^{-1}_q.\n","\n","        * The transform method maps the data into the feature space underlying\n","        the Nystrom-approximated kernel.\n","\n","        * The method K_approx directly computes the Nystrom approximation.\n","\n","        Parameters:\n","\n","        n_components [int] = how many samples to select from data.\n","        kernel [str] = type of kernel to use. Current options = {linear, rbf}.\n","        gamma [float] = exponential constant for the RBF kernel. \n","        random_state=[None, float] = to set a random seed for the random\n","                                     sampling of the samples. To be used when \n","                                     reproducibility is needed.\n","\n","    '''\n","  \n","    def __init__(self, n_components=100, kernel='linear',  gamma:float = 1., \n","                 random_state=None ):\n","        \n","        self.n_components = n_components\n","        self.kernel = kernel\n","        self.random_state = random_state\n","        self.gamma = gamma\n","\n","\n","    def fit(self, X:LazyTensor):\n","        ''' \n","        Args:   X = torch lazy tensor with features of shape \n","                (1, n_samples, n_features)\n","\n","        Returns: Fitted instance of the class\n","        '''\n","\n","        # Basic checks: we have a lazy tensor and n_components isn't too large\n","        assert type(X) == LazyTensor, 'Input to fit(.) must be a LazyTensor.'\n","        assert X.shape[1] >= self.n_components, f'The application needs X.shape[1] >= n_components.'\n","\n","        X = X.sum(dim=0) \n","        # Number of samples\n","        n_samples = X.size(0)\n","        # Define basis\n","        rnd = check_random_state(self.random_state)\n","        inds = rnd.permutation(n_samples)\n","        basis_inds = inds[:self.n_components]\n","        basis = X[basis_inds]\n","        # Build smaller kernel\n","        basis_kernel = self._pairwise_kernels(basis, kernel = self.kernel)  \n","        # Get SVD\n","        U, S, V = torch.svd(basis_kernel)\n","        S = torch.maximum(S, torch.ones(S.size()) * 1e-12)\n","        self.normalization_ = torch.mm(U / np.sqrt(S), V.t())\n","        self.components_ = basis\n","        self.component_indices_ = inds\n","        \n","        return self\n","\n","\n","    def _pairwise_kernels(self, x:torch.tensor, y:torch.tensor = None, kernel='linear',\n","                          gamma = 1.) -> torch.tensor:\n","        '''Helper function to build kernel\n","        \n","        Args:   X = torch tensor of dimension 2.\n","                K_type = type of Kernel to return\n","        '''\n","        \n","        if y is None:\n","            y = x\n","        if kernel == 'linear':\n","            K = x @ y.T\n","        elif kernel == 'rbf':\n","            K =  ( (x[:,None,:] - y[None,:,:])**2 ).sum(-1)\n","            K = torch.exp(- gamma * K )\n","\n","        return K\n","\n","    def transform(self, X:LazyTensor) -> LazyTensor:\n","        ''' Applies transform on the data.\n","        \n","        Args:\n","            X [LazyTensor] = data to transform\n","        Returns\n","            X [LazyTensor] = data after transformation\n","        '''\n","        \n","        X = X.sum(dim=0)\n","        K_nq = self._pairwise_kernels(X, self.components_, self.kernel)\n","        return LazyTensor((K_nq @ self.normalization_.t())[None,:,:])\n","\n","    \n","    def K_approx(self, X:LazyTensor) -> LazyTensor:\n","        ''' Function to return Nystrom approximation to the kernel.\n","        \n","        Args:\n","            X[LazyTensor] = data used in fit(.) function.\n","        Returns\n","            K[LazyTensor] = Nystrom approximation to kernel'''\n","        \n","        X = X.sum(dim=0)\n","        K_nq = self._pairwise_kernels(X, self.components_, self.kernel)\n","        K_approx = K_nq @ self.normalization_ @ K_nq.t()\n","        return LazyTensor(K_approx[None,:,:])\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6bh7DHDWND2E"},"source":["## Testing the fit and transform methods - numpy version\n","\n","Note: Given $a$ and $b$ two vectors, I am computing the error as $$e = \\frac{\\| a- b\\|_2}{L} $$ where $L = len(a)$. My reasoning for the $1/L$ factor is that the error will grow linearly with the size of the vectors. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0jLOzI1QNWdj","executionInfo":{"status":"ok","timestamp":1612093152362,"user_tz":-120,"elapsed":967,"user":{"displayName":"Yaniel Cabrera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnuKAiqQmMSKvIwoiQyk5G_g5QALh-semcxgjuQ6o=s64","userId":"16420725285670537399"}},"outputId":"cdbc54b0-3ecf-4d4f-f7db-d485443f7b51"},"source":["# We test the LazyNystrom_N fit/transform methods using a Linear kernel\n","\n","length = 1000\n","num_sampling = 100\n","\n","x = torch.randint(10,(1,length,3),dtype=torch.float32)\n","X_i = LazyTensor(x)\n","\n","# Instatiate & fit Nystroem for comparison\n","sk_N = Nystroem(kernel='linear', n_components=num_sampling, random_state=0).fit(x[0].numpy())  # input: (length, features) array\n","x_new = sk_N.transform(x[0].numpy())                                                           # output: (length, num_sampling) array\n","\n","# Instatiate & fit on lazy tensor version\n","LN_test = LazyNystrom_N(num_sampling, random_state=0).fit(X_i)   # input: (1, length, features) lazy tensor\n","X_new_i = LN_test.transform(X_i)                                 # output: (1,length,num_sampling) lazy tensor\n","\n","# Print the L2 error\n","err = np.linalg.norm(x_new - X_new_i.sum(dim=0).numpy()) / x_new.size\n","print(f'Error when compared to sklearn = {err}')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Error when compared to sklearn = 0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7WZo3i-9NcB0","executionInfo":{"status":"ok","timestamp":1612093161429,"user_tz":-120,"elapsed":988,"user":{"displayName":"Yaniel Cabrera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnuKAiqQmMSKvIwoiQyk5G_g5QALh-semcxgjuQ6o=s64","userId":"16420725285670537399"}},"outputId":"a79700f4-7d36-4bef-afba-aff29cdf21dd"},"source":["# We test the LazyNystrom_N fit/transform methods using a Gaussian kernel\n","\n","length = 1000\n","num_sampling = 100\n","\n","x = torch.randint(10,(1,length,3),dtype=torch.float32)\n","X_i = LazyTensor(x)\n","\n","# Instatiate & fit Nystroem for comparison\n","sk_N = Nystroem(kernel='rbf', gamma=1., n_components=num_sampling, random_state=0).fit(x[0].numpy())\n","x_new = sk_N.transform(x[0].numpy())      # (length, num_sampling) array\n","\n","# Instatiate & fit on lazy tensor version\n","LN_test = LazyNystrom_N(num_sampling,kernel='rbf', gamma=1., random_state=0).fit(X_i) # input: (1, length, features) lazy tensor\n","X_new_i = LN_test.transform(X_i)                                                      # output: (1,length,num_sampling) lazy tensor\n","\n","# Print the L2 error\n","err = np.linalg.norm(x_new - X_new_i.sum(dim=0).numpy()) / x_new.size\n","print(f'Error when compared to sklearn =  {err}')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Error when compared to sklearn =  0.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SNyU-oGFNocH"},"source":["## Testing the fit and transform methods - torch version"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-AqnQq0cNtFX","executionInfo":{"status":"ok","timestamp":1612093171751,"user_tz":-120,"elapsed":957,"user":{"displayName":"Yaniel Cabrera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnuKAiqQmMSKvIwoiQyk5G_g5QALh-semcxgjuQ6o=s64","userId":"16420725285670537399"}},"outputId":"50535e6c-8084-4823-e8b7-7719f29c88f5"},"source":["# We test the LazyNystrom_T fit/transform methods using a Linear kernel\n","\n","length = 1000\n","num_sampling = 100\n","\n","x = torch.randint(10,(1,length,3),dtype=torch.float32)\n","X_i = LazyTensor(x)\n","\n","# Instatiate & fit Nystroem for comparison\n","sk_N = Nystroem(kernel='linear', n_components=num_sampling, random_state=0).fit(x[0].numpy())\n","x_new = sk_N.transform(x[0].numpy())      # (length, num_sampling) array\n","\n","# Instatiate & fit on lazy tensor version\n","LN_test = LazyNystrom_T(num_sampling, random_state=0).fit(X_i)                   # input: (1, length, features) lazy tensor\n","X_new_i = LN_test.transform(X_i)                                                 # output: (1,length,num_sampling) lazy tensor\n","\n","# Print the L2 error\n","err = np.linalg.norm(x_new - X_new_i.sum(dim=0).numpy()) / x_new.size\n","print(f'Error when compared to sklearn = {err}')"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Error when compared to sklearn = 7.349115610122681e-06\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Z5-PKfFNxsa","executionInfo":{"status":"ok","timestamp":1612093181735,"user_tz":-120,"elapsed":931,"user":{"displayName":"Yaniel Cabrera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnuKAiqQmMSKvIwoiQyk5G_g5QALh-semcxgjuQ6o=s64","userId":"16420725285670537399"}},"outputId":"4795ade7-15dd-45fc-a7a6-6f296581af38"},"source":["# We test the LazyNystrom_T fit/transform methods using a Gaussian kernel\n","\n","length = 1000\n","num_sampling = 100\n","\n","x = torch.randint(10,(1,length,3),dtype=torch.float32)\n","X_i = LazyTensor(x)\n","\n","# Instatiate & fit Nystroem for comparison\n","sk_N = Nystroem(kernel='rbf', n_components=num_sampling, random_state=0).fit(x[0].numpy())\n","x_new = sk_N.transform(x[0].numpy())      # (length, num_sampling) array\n","\n","# Instatiate & fit on lazy tensor version\n","LN_test = LazyNystrom_T(num_sampling, kernel='rbf', random_state=0).fit(X_i)\n","X_new_i = LN_test.transform(X_i)          # (1,length,num_sampling) lazy tensor\n","\n","# Print the L2 error\n","err = np.linalg.norm(x_new - X_new_i.sum(dim=0).numpy()) / x_new.size\n","print(f'Error when compared to sklearn = {err}')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Error when compared to sklearn = 0.0001354495906829834\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_ihwunzVN2TE"},"source":[""],"execution_count":null,"outputs":[]}]}